---
title: CaTARo - Simulated Patient Robot
author: webstaff
lang: jp
tags:
    - generative modeling
    - simulated patient
    - virtual reality
    - augmented reality
---


{% case site.lang %}
{% when 'en' %}

# Care-giving Training Assistant Robot (CaTARo)

## Background

Japan is facing one of the most rapid aging trends in the world, with nearly 30% of its population aged 65 or older. This demographic shift is creating unprecedented demand for healthcare services, while simultaneously exacerbating shortages of medical professionals and caregivers. Traditional training methods alone are no longer sufficient to prepare enough skilled personnel to meet the complex medical and caregiving needs of an aging society. In this context, simulated patient robots offer a critical solution by providing realistic, repeatable, and safe training environments. 

Unlike human actors or mannequins, these robots can consistently reproduce a wide range of physical and behavioral symptoms, enabling healthcare trainees to practice and refine their skills without risk to real patients. Moreover, by incorporating sensors and data-driven feedback, simulated robots can objectively assess trainee performance, helping to standardize education quality and accelerate the development of clinical competence.

## Physical Robot for Rehabilitation Exercise

CaTARo features motorized arms that can replicate human-like movements, allowing trainees to practice range-of-motion exercises and other physical interactions. The robot's head unit includes a 3D facial avatar capable of expressing pain levels, which is projected onto a facial mask to provide visual feedback.

{%
  include figure.html
  image="/images/cataro/system_overview2.jpg"
  caption="CaTARo V1 (2021): shoulder complex, elbow, and arm modules."
  link="team"
  width="800px"
%}

{% capture col1 %}
{%
  include figure.html
  image="https://www.tekscan.com/sites/default/files/a201-main.jpg"
  caption="FlexiForce A201"
  link="team"
  width="300px"
%}
{%
  include figure.html
  image="https://i.ebayimg.com/00/s/MTYwMFgxMjA0/z/C08AAOSwK~lnLAe-/$_57.JPG?set_id=880000500F"
  caption="MX-28, Robotis"
  link="team"
  width="300px"
%}
{% endcapture %}
{% capture col2 %}

CaTARo's shoulder complex is designed with multiple degrees of freedom to replicate the intricate movements of the human shoulder. This includes the glenohumeral and sternoclavicular joints, enabling realistic simulations of various arm motions essential for caregiver training.

The robot employs high-precision servo motors, such as the MX-28 from Robotics Inc., to drive its joints. These actuators provide the necessary torque and control to mimic human-like movements accurately. ​

### Hardwares

- Servo motors: MX-28
- Sensors
  - Angle sensor: SV01L103AEA11T00
  - Force sensor: CFS018CA201
  - 4 x FlexiForce A201 


{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

## Simulated Patient Avatar

By utilizing data-driven models and real-time feedback mechanisms, the avatar can mimic expressions associated with pain, discomfort, or other emotional states, providing trainees with a more immersive and authentic learning experience. Such realistic simulations are crucial for developing empathy, improving diagnostic skills, and preparing healthcare providers for real-world patient interactions.

Our research provides experimental solutions for both virtual and physical environments.

### A. Mixed Reality

{% capture col2 %}
{%
  include figure.html
  image="/images/cataro/cataro_vr.webp"
  caption="CaTARo VR powered by Unity"
  link="/images/cataro/cataro_vr.webp"
  width="400px"
%}
{% endcapture %}
{% capture col1 %}

The integration of Virtual Reality (VR) with robotic limbs offers a powerful platform for immersive medical training. In this system, the robot's physical limbs are synchronized with a virtual avatar in a pre-designed interactive scenario, allowing trainees to practice procedures in a highly controlled and responsive environment.

As the robot moves, its joint data is streamed to update the avatar in real time, ensuring that physical performance is accurately mirrored in the virtual environment. By combining physical embodiment with VR-based contextual training, this approach enhances skill acquisition, procedural accuracy, and trainee confidence, while providing a safe, repeatable, and measurable environment critical for preparing healthcare providers.


{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

### B. Physical Embodiement

{%
  include figure.html
  image="/images/cataro/interface_compare.png"
  caption="Comparison between experimental mediums for simulated patient facial communication"
  link="/images/cataro/interface_compare.svg"
  width="600px"
  no_shadow=true
%}

#### 1. Back-projection Display

CaTARo features a 3D facial avatar system that projects expressions onto a facial mask. This system utilizes data from the UNBC-McMaster Pain Archive and a fuzzy logic–based evaluation method to display real-time pain expressions, enhancing the realism of training scenarios.

{%
  include figure.html
  image="/images/cataro/system_overview.jpg"
  caption="Pained expression with back-projection mechanism"
  link="team"
  width="600px"
%}

#### 2. Hyper-realistic 3D Gaussian Avatar

By employing a mesh priori, we trained a 3D Gaussian Splatting-based avatar that synthesizes facial animation in real-time. The facial expression can be transferred and control through FLAME parameters that constructs the prior mesh. However, as the expression spaces differ across subjects, we are also researching how to disentangle the universal and unique expression space to better capture human facial expression.

{%
  include figure.html
  image="/images/cataro/combined.gif"
  caption="Nersemble dataset: Left - Monocular tracked animation; Middle - 3D mesh tracking; Right - Synthesized animation"
  link="/images/cataro/combined.gif"
  width="600px"
  no_shadow="true"
%}

#### 3. Holographic Avatar

{% capture col1 %}

VR headset allows immersive experience to simulated training session with limitation to one user at a time. Holographic display elevates this issue by projecting 3D volumetric visual to multiple subjects at the same time.

With customized CUDA's kernel and optimized OpenGL-based pipeline, we stream 3D Gaussian Avatar animation to 3D display in real-time according to user's viewing angles.

{%
  include figure.html
  image="/images/cataro/hologram.png"
  caption="Holographic rendering pipeline based on 3D Gaussian Splatting"
  link="/images/cataro/combined.gif"
  width="600px"
  no_shadow="true"
%}

{% endcapture %}

{% capture col2 %}

{%
  include figure.html
  image="/images/cataro/holo3d.webp"
  caption="3D display projects different views on various angles"
  link="/images/cataro/holo3d.webp"
  width="400px"
  no_shadow="true"
%}

{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

## Humanoid Facial Motion Modeling

Realistic data-driven behavior imitation skill is critical for simulated patients to achieve its goal. In our study, we analyzed human's facial behaviors, especially non-verbal facial behavior to learn stochastic facial reaction that powers our humanoid robot's facial behavior.

### A. Generalized Non-verbal Facial Behavior

We represented humanoid face behavior as temporal change of 3D Morphable Model (3DMM) parameters while exploring this modality's unique characteristics such as sparsity, continuity, and its correlation with other modality such as speech and visual signal of another human partner.

<video poster="" id="tree" autoplay="" muted="" loop="" width="800px"> <source src="/images/cataro/thumbnail_video.mp4" type="video/mp4"> </video>

The model extracts controlling time steps from transitioning steps to learn when, why, and how human reacts to stimulation in dyadic conversation as a listener.

### B. Painful Expression Behavior

To learn the relationship between pained expressions and pain intensity, we devised a generative learning model that predicts pained expression according to control signals such as pain intensity, emotional state, and tolerance.

<video poster="" id="tree" autoplay="" muted="" loop="" width="800px"> <source src="/images/cataro/pain_diff.webm" type="video/mp4"> </video>

{% when 'jp' %}

# 介護トレーニング支援ロボット（CaTARo）

## 背景

日本は世界でも最も急速な高齢化を迎えており、65歳以上の人口が全体の約30％を占めています。  
この人口動態の変化により、医療サービスに対する需要がかつてないほど高まる一方で、医療従事者や介護者の不足が深刻化しています。  
従来のトレーニング方法だけでは、複雑な医療・介護ニーズに対応できる十分な人材を育成することが困難になっています。  
このような状況において、模擬患者ロボットは、現実的かつ繰り返し可能で安全なトレーニング環境を提供する重要なソリューションとなります。

人間の俳優やマネキンとは異なり、これらのロボットは幅広い身体的・行動的症状を一貫して再現できるため、医療従事者の訓練生は、実際の患者にリスクを与えることなく技能を磨き、洗練することが可能です。  
さらに、センサーやデータ駆動型フィードバックを組み込むことで、訓練生のパフォーマンスを客観的に評価でき、教育の質の標準化と臨床能力の向上を加速させることができます。

## リハビリ訓練用物理ロボット

CaTARoは、人間のような動きを再現できるモーター駆動アームを搭載しており、訓練生は関節可動域訓練やその他の身体的インタラクションを実践できます。  
ヘッドユニットには、痛みのレベルを表現できる3Dフェイシャルアバターが搭載されており、顔マスクに投影して視覚的フィードバックを提供します。

{%
  include figure.html
  image="/images/cataro/system_overview2.jpg"
  caption="CaTARo V1 (2021): 肩複合関節、肘、腕モジュール"
  link="team"
  width="800px"
%}

{% capture col1 %}
{%
  include figure.html
  image="https://www.tekscan.com/sites/default/files/a201-main.jpg"
  caption="FlexiForce A201"
  link="team"
  width="300px"
%}
{%
  include figure.html
  image="https://i.ebayimg.com/00/s/MTYwMFgxMjA0/z/C08AAOSwK~lnLAe-/$_57.JPG?set_id=880000500F"
  caption="MX-28, Robotis"
  link="team"
  width="300px"
%}
{% endcapture %}
{% capture col2 %}

CaTARoの肩複合部は、多自由度設計により人間の肩の複雑な動きを再現できるよう設計されています。  
肩甲上腕関節（glenohumeral joint）や胸鎖関節（sternoclavicular joint）を含み、介護訓練に必要な多様な腕の動作をリアルにシミュレーション可能です。

ロボットの関節駆動には、Robotics Inc.製のMX-28のような高精度サーボモーターを採用しています。  
これらのアクチュエータは、人間らしい動きを正確に再現するために必要なトルクと制御性を提供します。

### ハードウェア構成

- サーボモーター: MX-28
- センサー
  - 角度センサー: SV01L103AEA11T00
  - 力覚センサー: CFS018CA201
  - フレキシフォースセンサー A201 ×4個

{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

## 模擬患者アバター

データ駆動型モデルとリアルタイムフィードバック機構を活用することで、アバターは痛み、不快感、その他の感情状態を模倣し、訓練生により没入感のあるリアルな学習体験を提供します。  
このようなリアリティのあるシミュレーションは、共感力の育成、診断スキルの向上、実際の患者対応能力の強化に不可欠です。

本研究では、仮想環境と物理環境の両方に対応した実験的ソリューションを提供しています。

### A. 複合現実（Mixed Reality）

{% capture col2 %}
{%
  include figure.html
  image="/images/cataro/cataro_vr.webp"
  caption="CaTARo VR (Unityベース)"
  link="/images/cataro/cataro_vr.webp"
  width="400px"
%}
{% endcapture %}
{% capture col1 %}

VR（バーチャルリアリティ）とロボットアームの統合により、没入型医療訓練のための強力なプラットフォームが構築されます。  
本システムでは、ロボットの物理的な動きが事前設計されたインタラクティブな仮想シナリオと同期され、訓練生は高度に制御された環境下で手技を実践できます。

ロボットが動くたびに関節データがリアルタイムでアバターに反映され、仮想環境における動作の正確な再現が可能です。  
物理的実体とVRベースの文脈的訓練を組み合わせることで、スキル習得、手技精度、訓練生の自信向上が促進され、安全かつ再現可能で測定可能な環境が提供されます。

{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

### B. 物理的実体化（Physical Embodiment）

{%
  include figure.html
  image="/images/cataro/interface_compare.png"
  caption="模擬患者の顔表現における実験媒体の比較"
  link="/images/cataro/interface_compare.svg"
  width="600px"
  no_shadow=true
%}

#### 1. 背面投影ディスプレイ（Back-projection Display）

CaTARoには、顔マスクに表情を投影する3Dフェイシャルアバターシステムが搭載されています。  
このシステムは、UNBC-McMaster Pain Archiveのデータとファジィ論理に基づく評価手法を用い、リアルタイムで痛みの表情を再現し、トレーニングシナリオのリアリズムを向上させます。

{%
  include figure.html
  image="/images/cataro/system_overview.jpg"
  caption="背面投影機構による痛み表現"
  link="team"
  width="600px"
%}

#### 2. 超リアルな3Dガウシアンアバター（Hyper-realistic 3D Gaussian Avatar）

事前メッシュ（priori mesh）を用いて、リアルタイムで顔アニメーションを生成できる3D Gaussian Splattingベースのアバターを学習させました。  
表情はFLAMEパラメータを通じて制御・転送されます。  
ただし、被験者ごとに表情空間が異なるため、より人間らしい表情再現のため、共通表現空間と個別表現空間の分離に関する研究も進めています。

{%
  include figure.html
  image="/images/cataro/combined.gif"
  caption="Nersembleデータセット：左 - 単眼トラッキング、中央 - 3Dメッシュトラッキング、右 - 合成アニメーション"
  link="/images/cataro/combined.gif"
  width="600px"
  no_shadow="true"
%}

#### 3. ホログラフィックアバター（Holographic Avatar）

{% capture col1 %}

VRヘッドセットは没入型体験を提供しますが、同時に一人しか利用できないという制約があります。  
ホログラフィックディスプレイは、3Dボリューム映像を複数の人に同時に投影することで、この問題を解決します。

CUDAカーネルと最適化されたOpenGLパイプラインを活用し、ユーザーの視点角度に応じて3D Gaussianアバターアニメーションをリアルタイムで3Dディスプレイにストリーミングしています。

{%
  include figure.html
  image="/images/cataro/hologram.png"
  caption="3D Gaussian Splattingに基づくホログラフィックレンダリングパイプライン"
  link="/images/cataro/combined.gif"
  width="600px"
  no_shadow="true"
%}

{% endcapture %}

{% capture col2 %}

{%
  include figure.html
  image="/images/cataro/holo3d.webp"
  caption="異なる角度に異なる映像を投影する3Dディスプレイ"
  link="/images/cataro/holo3d.webp"
  width="400px"
  no_shadow="true"
%}

{% endcapture %}

{%
  include cols.html
  col1=col1
  col2=col2
%}

## ヒューマノイド顔面動作モデリング

リアルなデータ駆動型行動模倣技術は、模擬患者ロボットが本来の目的を達成するために重要です。  
本研究では、人間の非言語的顔面行動に着目し、リスナーとしての顔面反応パターンを学習し、ロボットの表情制御に応用しています。

### A. 一般化された非言語顔面行動

ヒューマノイドの顔動作を3Dモーフィングモデル（3DMM）パラメータの時間変化として表現し、  
スパース性、連続性、また他者の音声・視覚信号との相関など、このモダリティ特有の特性を分析しました。

<video poster="" id="tree" autoplay="" muted="" loop="" width="800px"> <source src="/images/cataro/thumbnail_video.mp4" type="video/mp4"> </video>

モデルは、会話中に人間がどのタイミングで、なぜ、どのように反応するかを学習し、  
適切なリスナー反応生成を実現します。

### B. 痛み表現行動（Painful Expression Behavior）

痛み表現と痛み強度の関係性を学習するため、  
痛み強度、感情状態、耐性といった制御信号に応じて痛み表現を予測する生成モデルを開発しました。

<video poster="" id="tree" autoplay="" muted="" loop="" width="800px"> <source src="/images/cataro/pain_diff.webm" type="video/webm"> </video>

{% endcase %}
